'''
Averaging the viterbi decoding.

How we do it:
The viterbi decoding itself cannot be averaged, it is discrete.
We average the emissions of multiple models, and we average the
weights of the CRF for all models. Then we run the viterbi decoding
to get one single path, that we consider the average of the models.

'''
import torch
import os
import sys
sys.path.append("/zhome/1d/8/153438/experiments/master-thesis/") 
from typing import List, Tuple
import argparse

from models.multi_crf_bert import BertSequenceTaggingCRF, ProteinBertTokenizer
from train_scripts.utils.signalp_dataset import RegionCRFDataset
from tqdm import tqdm
import pandas as pd

device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')

def get_averaged_emissions(model_checkpoint_list: List[str], dataloader) -> Tuple[torch.Tensor, torch.Tensor, torch.Tensor]:
    '''Run sequences through all models in list.
    Return only the emissions (Bert+linear projection) and average those.
    Also returns input masks, as CRF needs them when decoding.
    '''

    emission_list = []
    global_probs_list = []
    for checkpoint in tqdm(model_checkpoint_list):

        # Load model and get emissions
        model = BertSequenceTaggingCRF.from_pretrained(checkpoint)
        model.to(device)
        model.eval()

        emissions_batched = []
        masks_batched = []
        probs_batched = []
        for i, batch in enumerate(dataloader):
            
            data, targets, input_mask, global_targets, weights,kingdoms, cleavage_sites= batch
            data = data.to(device)
            input_mask = input_mask.to(device)

            with torch.no_grad():
                global_probs, pos_probs, pos_preds, emissions, input_mask = model(data, input_mask=input_mask, return_emissions=True)
                emissions_batched.append(emissions)
                masks_batched.append(input_mask.cpu())
                probs_batched.append(global_probs.cpu())

        #covert to CPU tensors after forward pass. Save memory.
        model_emissions = torch.cat(emissions_batched).detach().cpu()
        emission_list.append(model_emissions)
        masks =  torch.cat(masks_batched) # gathering mask in inner loop is sufficent, same every time.
        model_probs = torch.cat(probs_batched).detach().cpu()
        global_probs_list.append(model_probs)

    # Average the emissions

    emissions = torch.stack(emission_list)
    probs = torch.stack(global_probs_list)
    return emissions.mean(dim=0), masks, probs.mean(dim=0)



def run_averaged_crf(model_checkpoint_list: List[str], emissions: torch.Tensor, input_mask: torch.tensor):
    '''Average weights of a list of model checkpoints,
    then run viterbi decoding on provided emissions.
    Running this on CPU should be fine, viterbi decoding
    does not use a lot of multiplications. Only one for each timestep.
    '''

    # Gather the CRF weights
    start_transitions = [] 
    transitions = []
    end_transitions = []

    for checkpoint in model_checkpoint_list:
        model = BertSequenceTaggingCRF.from_pretrained(checkpoint)
        start_transitions.append(model.crf.start_transitions.data)
        transitions.append(model.crf.transitions.data)
        end_transitions.append(model.crf.end_transitions.data)


    # Average and set model weights
    start_transitions = torch.stack(start_transitions).mean(dim=0)
    transitions = torch.stack(transitions).mean(dim=0)
    end_transitions = torch.stack(end_transitions).mean(dim=0)

    model.crf.start_transitions.data = start_transitions
    model.crf.transitions.data =  transitions
    model.crf.end_transitions.data = end_transitions

    # Get viterbi decodings
    with torch.no_grad():
        viterbi_paths = model.crf.decode(emissions=emissions, mask=input_mask.byte())

    return viterbi_paths


def main():

    parser = argparse.ArgumentParser()
    parser.add_argument('--data', type=str, default= '../data/signal_peptides/signalp_updated_data/signalp_6_train_set.fasta')
    parser.add_argument('--model_base_path', type=str, default = '/work3/felteu/tagging_checkpoints/signalp_6')
    parser.add_argument('--output_file', type=str, default = 'viterbi_paths.csv')

    args = parser.parse_args()

    tokenizer=ProteinBertTokenizer.from_pretrained("/zhome/1d/8/153438/experiments/master-thesis/resources/vocab_with_kingdom", do_lower_case=False)


    # Collect results + header info to build output df
    results_list = []

    partitions = [0,1,2,3,4]
    for partition in partitions:
        # Load data
        dataset = RegionCRFDataset(args.data, tokenizer=tokenizer, partition_id = [partition], add_special_tokens=True)
        dl = torch.utils.data.DataLoader(dataset, collate_fn = dataset.collate_fn, batch_size =100)

        # Put together list of checkpoints
        checkpoints = [os.path.join(args.model_base_path, f'test_{partition}_val_{x}') for x in set(partitions).difference({partition})]

        # Run
        emissions, input_mask, probs = get_averaged_emissions(checkpoints,dl)
        viterbi_paths =  run_averaged_crf(checkpoints,emissions,input_mask)

        # Gather results + info
        for i in range(len(dataset)):
            entrydict = {}
            entry, kingdom, typ, _ =dataset.identifiers[i].split('|')
            entrydict['Entry'] = entry
            entrydict['Kingdom'] = kingdom
            entrydict['Type'] = typ
            entrydict['Path'] = viterbi_paths[i]
            entrydict['Sequence'] = dataset.sequences[i]
            entrydict['True path'] = dataset.labels[i]
            entrydict['Pred label'] = probs[i].argmax().detach().numpy()

            results_list.append(entrydict)


    # Put back together and save
    df = pd.DataFrame.from_dict(results_list)

    df.to_csv(args.output_file)

if __name__ == '__main__':
    main()